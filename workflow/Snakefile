#!/usr/bin/env snakemake

# ───────────────────────────────────────────────
# IMPORT 
# ───────────────────────────────────────────────

import pandas as pd
import os
import sys

from collections import defaultdict

# ───────────────────────────────────────────────
# CONFIG 
# ───────────────────────────────────────────────

# Define minimum snakemake version
min_snakemake_version = "8.25.0"

# Load configuration from config file
configfile: ''.join([os.environ['GIT_REPODIR'], "/config/config.yaml"])


# ───────────────────────────────────────────────
# INPUT CSV VALIDATION 
# ───────────────────────────────────────────────

sample_csv_path = config.get("samples")
if not sample_csv_path or not os.path.isfile(sample_csv_path):
    sys.exit(f"[ERROR] Sample CSV file not found or missing: {sample_csv_path}")

# Read samples file and determine if we're dealing with bam or fastq inputs
samples_df = pd.read_csv(sample_csv_path)

bam_col = "path_to_bam"
fq_col  = "path_to_fastq"
has_bam = bam_col in samples_df.columns
has_fq  = fq_col  in samples_df.columns

if has_bam == has_fq:  # both True or both False
    sys.exit(f"[ERROR] CSV must contain exactly one of '{bam_col}' or '{fq_col}'.")

MODE = "bam" if has_bam else "fastq"

# Required columns
if "sample" not in samples_df.columns:
    sys.exit("[ERROR] CSV must contain column: 'sample'")

# Validate files & extensions
col = bam_col if MODE == "bam" else fq_col
if samples_df[col].isna().any():
    sys.exit(f"[ERROR] Empty paths in column '{col}' are not allowed.")
missing = samples_df.loc[~samples_df[col].apply(os.path.exists), col]
if not missing.empty:
    sys.exit(f"[ERROR] These files from '{col}' do not exist:\n  " + "\n  ".join(map(str, missing.tolist())))

if MODE == "bam":
    bad_ext = samples_df[col].map(lambda p: not str(p).endswith(".bam"))
    if bad_ext.any():
        bad = samples_df.loc[bad_ext, col].astype(str).tolist()
        sys.exit("[ERROR] Non-.bam paths in bam mode:\n  " + "\n  ".join(bad))
else:
    bad_ext = samples_df[col].map(lambda p: not (str(p).endswith(".fastq") or str(p).endswith(".fastq.gz")))
    if bad_ext.any():
        bad = samples_df.loc[bad_ext, col].astype(str).tolist()
        sys.exit("[ERROR] Non-fastq(.gz) paths in fastq mode:\n  " + "\n  ".join(bad))

print(f"[INFO] Sample sheet validation passed. Mode: {MODE}", file=sys.stderr)


# ───────────────────────────────────────────────
# LOCAL VARIABLE DEFINITIONS 
# ───────────────────────────────────────────────

# Utility function to make sure that values we expect to be booleans are passed as such
def as_bool(val):
    if isinstance(val, bool):
        return val
    if isinstance(val, str):
        return val.strip().lower() in {"true", "yes", "1"}
    return False

# Set snakemake main workdir variable. 
SNAKEDIR = os.path.dirname(workflow.snakefile) + "/"

# Map samples -> list of source files
PARTS = defaultdict(list)      
SRC_COL = bam_col if MODE == "bam" else fq_col
for row in samples_df.itertuples(index=False):
    PARTS[row.sample].append(getattr(row, SRC_COL))
PART_MAP = {(s, i): p for s, lst in PARTS.items() for i, p in enumerate(lst)}


# Define local variables: General
SAMPLES         = sorted(PARTS.keys())
MERGEDISOPREFIX = config.get("merged_isoform_prefix", "ISOP")
MAXINTRONLEN    = config.get("max_intron_length", 1250000)
USE_TC          = as_bool(config.get("use_transcriptclean", True))
ONLYCHR         = as_bool(config.get("limit_output_to_chrNXY", True))
MAXCHRMREADS    = config.get("max_chrM_reads", 50000)

# Define local variables: Reference files
GENOMEFASTA     = config.get("reference_genome_fasta")
REFGTF          = config.get("reference_annotations_gtf")
REFTSS          = config.get("reference_tss_bed")
SPLICEJUNCTIONS = config.get("splice_junctions_file")
TERMINALEXONBED = config.get("reference_intrapriming_exclude_regions_bed","")
RMSKBED         = config.get("reference_repeat_regions_bed")
PARBED          = config.get("reference_PAR_regions_bed")
SEGDUP          = config.get("reference_segdups_bed")

# Define local variables: Filter thresholds
FILT_MONOEXON_TSS_MAXDIST_BP      = config.get("filt_monoexon_tss_maxdist_bp", 10)
FILT_MONOEXON_MIN_INTRON_OVLP_BP  = config.get("filt_monoexon_min_intron_ovlp_bp", 10)
FILT_RMSK_MIN_OVLP_FRACT          = config.get("filt_rmsk_min_ovlp_fract", 0.9)
FILT_PAR_MIN_OVLP_FRACT           = config.get("filt_par_min_ovlp_fract", 0)
FILT_TPM_MIN_COUNT                = config.get("filt_tpm_min_count", 10)
FILT_TPM_MIN_SAMPLES              = config.get("filt_tpm_min_samples", 3)

# Define the filter tag used to mark subdirectories and group outputs
FILTERTAG = f"tpm{FILT_TPM_MIN_COUNT}s{FILT_TPM_MIN_SAMPLES}"

# Parameters for merging fully contained splicechains
CONSOLIDATE_CONTAINED_SPLICECHAINS  = as_bool(config.get("consolidate_contained_splicechains", True))
CONSOLIDATE_PROTECT_WINDOW          = int(config.get("consolidate_protect_window", 75))
CONSOLIDATE_ROUND_MODE              = config.get("consolidate_round_mode","nearest")
CONSOLIDATE_MERGE_MIN_FRAC          = config.get("consolidate_merge_min_frac",0.1)
CONSOLIDATE_MERGE_MIN_SAMPLES       = config.get("consolidate_merge_min_samples",2)

# Parameters for pruning low-expressed isoforms
PRUNE_LOW_EXPRESSED_ISOFORMS_RETAIN_PCT = config.get("prune_low_expressed_isoforms_retain_pct",97)
PRUNE_LOW_EXPRESSED_ISFORMS_MATCH_MODE  = config.get("prune_low_expressed_isforms_match_mode","any_shared")
PRUNE_LOW_EXPRESSED_ISFORMS_MIN_SAMPLES = config.get("prune_low_expressed_isforms_min_samples",2)

# Define local variables: Choose whether or not to run certain filter rules
REMOVE_MONOEXONS_NO_TSS       = as_bool(config.get("remove_monoexons_without_cage_support", True))
REMOVE_MONOEXON_PRE_MRNAS     = as_bool(config.get("remove_monoexon_premRNA_fragments", True))
REMOVE_NONCANONICAL_SPLICE    = as_bool(config.get("remove_isoforms_with_noncanonical_splice_junctions", True))
REMOVE_TSWITCH_ARTIFACTS      = as_bool(config.get("remove_isoforms_with_template_switching_artifacts", True))
REMOVE_ANTISENSE_SPLICEMATCH  = as_bool(config.get("remove_isoforms_with_antisense_perfect_splicematch", True))
REMOVE_CONTAINED_IN_REPEATS   = as_bool(config.get("remove_isoforms_fully_contained_in_repeats", True))
REMOVE_PAR_OVERLAP            = as_bool(config.get("remove_isoforms_overlapping_PAR_regions", True))
REMOVE_BELOW_TPM              = as_bool(config.get("remove_isoforms_not_meeting_TPM_threshold", True))
REMOVE_TERMINAL_EXONS_SEGDUP  = as_bool(config.get("remove_isoforms_with_mismapped_segdup_exons", True))

# Define local variables: transcriptclean chunking parameters
TRANSCRIPTCLEAN_CHUNK_MAXREADS  = config.get("transcriptclean_chunk_maxreads",2000000)
TRANSCRIPTCLEAN_CHUNK_THREADS   = config.get("transcripclean_chunk_threads",2)

# QC local variables
BAM_CHOICE          = str(config.get("qc_bam_choice", "labeled")).lower()  # "labeled" | "mapped"
VARSCAN_MIN_COV     = config.get("varscan_min_coverage", 20),
VARSCAN_MIN_READS2  = config.get("varscan_min_reads2", 4),

# Define local variables: pbfusion arguments
RUN_PBFUSION                        = as_bool(config.get("run_pbfusion", True))
PBFUSION_MIN_FUSION_QUALITY         = config.get("pbfusion_min_fusion_quality", "MEDIUM")
PBFUSION_MIN_COVERAGE               = config.get("pbfusion_min_coverage", 2)
PBFUSION_MIN_MEAN_IDENTITY          = config.get("pbfusion_min_mean_identity", 0.93)
PBFUSION_MIN_MEAN_MAPQ              = config.get("pbfusion_min_mean_mapq", 10)
PBFUSION_MIN_FUSION_READ_FRACTION   = config.get("pbfusion_min_fusion_read_fraction", 0.25)
PBFUSION_MAX_VARIABILITY            = config.get("pbfusion_max_variability", 1000)
PBFUSION_MAX_READTHROUGH            = config.get("pbfusion_max_readthrough", 100000)
PBFUSION_MAX_GENES_IN_EVENT         = config.get("pbfusion_max_genes_in_event", 3)
PBFUSION_MIN_FUSION_FRACTION        = config.get("pbfusion_min_fusion_fraction", 0.01)
PBFUSION_PROM_FILTER                = config.get("pbfusion_prom_filter", 8)

# Set conditional isoPropeller arguments. If a terminal exon bed file is defined
# in the configuration file, use it.
if TERMINALEXONBED:
    ISOPROPEXTRAARGS = f"-l {TERMINALEXONBED}"
else:
    ISOPROPEXTRAARGS = ""


# ───────────────────────────────────────────────
# RULES 
# ───────────────────────────────────────────────

# Import common rules
include: "rules/general_rules.smk"
include: "rules/01_mapping.smk"
include: "rules/03_isopropeller.smk"
include: "rules/04_isopropeller-merge.smk"
include: "rules/05_isopropeller-filter.smk"
include: "rules/06_qc-reports.smk"
include: "rules/07_isopropeller-defrag.smk"
include: "rules/08_isopropeller-defrag-prune.smk"

# Import conditional rules
if MODE == "bam":
    print("[INFO] Loading BAM→FASTQ staging rules.", file=sys.stderr)
    include: "rules/00_bam-input.smk"
else:
    print("[INFO] Loading FASTQ staging rules.", file=sys.stderr)
    include: "rules/00_fastq-input.smk"

if USE_TC:
    print("[INFO] Loading conditional transcriptclean rules.", file=sys.stderr)
    include: "rules/02_transcriptclean.smk"

PBFUSION_TARGETS = []
if RUN_PBFUSION:
    print("[INFO] Loading conditional pbfusion rules.", file=sys.stderr)
    include: "rules/09_pbfusion.smk"
    
    # Include all final pbfusion targets
    PBFUSION_TARGETS = expand([
        "09_pbfusion/{sample}/{sample}.breakpoints.bed",
        "09_pbfusion/{sample}/{sample}.breakpoints.groups.bed",
        "09_pbfusion/{sample}/{sample}.transcripts",
        "09_pbfusion/{sample}/{sample}.unannotated.bed",
        "09_pbfusion/{sample}/{sample}.unannotated.clusters.bed"
    ], sample=SAMPLES)


# Make final targets
rule all:
    input:
        # Original pipeline outputs
        expand("05_isoPropeller-filter/{prefix}_depth-gt1_" + FILTERTAG + "/{prefix}_depth-gt1_isoqc_pass.gtf", prefix=[MERGEDISOPREFIX]),
        expand("05_isoPropeller-filter/{prefix}_depth-gt1_" + FILTERTAG + "/{prefix}_depth-gt1_isoqc_pass_exp.txt", prefix=[MERGEDISOPREFIX]),
        expand("05_isoPropeller-filter/{prefix}_depth-gt1_" + FILTERTAG + "/{prefix}_depth-gt1_isoqc_pass_id.txt", prefix=[MERGEDISOPREFIX]),
        expand("05_isoPropeller-filter/{prefix}_depth-gt1_" + FILTERTAG + "/{prefix}_depth-gt1_isoqc_pass_tss.bed", prefix=[MERGEDISOPREFIX]),
        expand("05_isoPropeller-filter/{prefix}_depth-gt1_" + FILTERTAG + "/{prefix}_depth-gt1_isoqc_pass_tts.bed", prefix=[MERGEDISOPREFIX]),
        expand("05_isoPropeller-filter/{prefix}_depth-gt1_" + FILTERTAG + "/{prefix}_depth-gt1_isoqc_pass.trackgroups", prefix=[MERGEDISOPREFIX]),

        # QC Seqkit on FLNC fastq
        "06_qc-reports/flnc-seqkit-stats/seqkit_flnc_wide.stats.tsv",

        # QC mapped BAM summaries (per-sample + cohort table)
        expand("06_qc-reports/mapped-bamqc/{sample}/summary.tsv", sample=SAMPLES),
        "06_qc-reports/mapped-bamqc/bam_qc_summary_long.tsv",
        "06_qc-reports/mapped-bamqc/bam_qc_summary_wide.tsv",

        # QC mapped RNA-SeQC metrics table
        "06_qc-reports/mapped-rnaseqc/rna_seqc_summary_long.tsv",
        "06_qc-reports/mapped-rnaseqc/rna_seqc_summary_wide.tsv",

        # QC mapped SNP genotypes
        expand("06_qc-reports/mapped-snp-genotypes/{sample}.varscan.snp.tsv", sample=SAMPLES),

        # QC MultiQC — single report
        "06_qc-reports/multiqc/multiqc_report.html",

        # QC longreasum, FASTQ report
        expand("06_qc-reports/flnc-longreadsum-fastq-report/{sample}", sample=SAMPLES),

        # QC longreadsum, RNA-seq BAM report
        expand("06_qc-reports/mapped-longreadsum-rnaseq-report/{sample}", sample=SAMPLES),

        # QC isoform filtering reports
        expand(
            "06_qc-reports/isoform-filtering/{prefix}_{suffix}_{filtertag}/isoform-filter-stats.totals.tsv",
            prefix=[MERGEDISOPREFIX],
            suffix=["depth-gt1"],
            filtertag=[FILTERTAG],
        ),
        expand(
            "06_qc-reports/isoform-filtering/{prefix}_{suffix}_{filtertag}/isoform-filter-stats.per-sample-wide.tsv",
            prefix=[MERGEDISOPREFIX],
            suffix=["depth-gt1"],
            filtertag=[FILTERTAG],
        ),

        # Defragmented pipeline outputs
        expand("07_isoPropeller-defrag/{prefix}_depth-gt1_" + FILTERTAG + "/{prefix}_depth-gt1_isoqc_pass_defrag_exp_redist.txt", prefix=[MERGEDISOPREFIX]),
        expand("07_isoPropeller-defrag/{prefix}_depth-gt1_" + FILTERTAG + "/{prefix}_depth-gt1_isoqc_pass_defrag.gtf", prefix=[MERGEDISOPREFIX]),
        expand("07_isoPropeller-defrag/{prefix}_depth-gt1_" + FILTERTAG + "/{prefix}_depth-gt1_isoqc_pass_defrag_exp.txt", prefix=[MERGEDISOPREFIX]),
        expand("07_isoPropeller-defrag/{prefix}_depth-gt1_" + FILTERTAG + "/{prefix}_depth-gt1_isoqc_pass_defrag_id.txt", prefix=[MERGEDISOPREFIX]),
        expand("07_isoPropeller-defrag/{prefix}_depth-gt1_" + FILTERTAG + "/{prefix}_depth-gt1_isoqc_pass_defrag_tss.bed", prefix=[MERGEDISOPREFIX]),
        expand("07_isoPropeller-defrag/{prefix}_depth-gt1_" + FILTERTAG + "/{prefix}_depth-gt1_isoqc_pass_defrag_tts.bed", prefix=[MERGEDISOPREFIX]),
        expand("07_isoPropeller-defrag/{prefix}_depth-gt1_" + FILTERTAG + "/{prefix}_depth-gt1_isoqc_pass_defrag.trackgroups", prefix=[MERGEDISOPREFIX]),

        # Defrag-pruned pipeline outputs
        expand("08_isoPropeller-defrag-pruned/{prefix}_depth-gt1_" + FILTERTAG + "/{prefix}_depth-gt1_isoqc_pass_defrag_pruned.gtf", prefix=[MERGEDISOPREFIX]),
        expand("08_isoPropeller-defrag-pruned/{prefix}_depth-gt1_" + FILTERTAG + "/{prefix}_depth-gt1_isoqc_pass_defrag_pruned_modal_ends.gtf", prefix=[MERGEDISOPREFIX]),
        expand("08_isoPropeller-defrag-pruned/{prefix}_depth-gt1_" + FILTERTAG + "/{prefix}_depth-gt1_isoqc_pass_defrag_pruned_exp.txt", prefix=[MERGEDISOPREFIX]),
        expand("08_isoPropeller-defrag-pruned/{prefix}_depth-gt1_" + FILTERTAG + "/{prefix}_depth-gt1_isoqc_pass_defrag_pruned_id.txt", prefix=[MERGEDISOPREFIX]),
        expand("08_isoPropeller-defrag-pruned/{prefix}_depth-gt1_" + FILTERTAG + "/{prefix}_depth-gt1_isoqc_pass_defrag_pruned_tss.bed", prefix=[MERGEDISOPREFIX]),
        expand("08_isoPropeller-defrag-pruned/{prefix}_depth-gt1_" + FILTERTAG + "/{prefix}_depth-gt1_isoqc_pass_defrag_pruned_tts.bed", prefix=[MERGEDISOPREFIX]),
        expand("08_isoPropeller-defrag-pruned/{prefix}_depth-gt1_" + FILTERTAG + "/{prefix}_depth-gt1_isoqc_pass_defrag_pruned.trackgroups", prefix=[MERGEDISOPREFIX]),
        expand("08_isoPropeller-defrag-pruned/{prefix}_depth-gt1_" + FILTERTAG + "/{prefix}_depth-gt1_isoqc_pass_defrag_pruned_clusters.txt", prefix=[MERGEDISOPREFIX]),
        expand("08_isoPropeller-defrag-pruned/{prefix}_depth-gt1_" + FILTERTAG + "/{prefix}_depth-gt1_isoqc_pass_defrag_pruned_dropped.txt", prefix=[MERGEDISOPREFIX]),

        # Conditional PBfusion targets
        PBFUSION_TARGETS
