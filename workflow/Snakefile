#!/usr/bin/env snakemake

##########
# IMPORT #
##########

import pandas as pd
import os
import sys

from collections import defaultdict

##########
# CONFIG #
##########

# Define minimum snakemake version
min_snakemake_version = "8.25.0"

# Load configuration from config file
configfile: ''.join([os.environ['GIT_REPODIR'], "/config/config.yaml"])


########################
# INPUT CSV VALIDATION #
########################

# Define expected columns in the CSV
expected_columns = {"path_to_flnc_bam", "sample"}
sample_csv_path  = config.get("samples")

# Load and validate sample sheet
if not sample_csv_path or not os.path.isfile(sample_csv_path):
    sys.exit(f"[ERROR] Sample CSV file not found or missing: {sample_csv_path}")

samples_df = pd.read_csv(sample_csv_path)

# Check required headers
if not expected_columns.issubset(samples_df.columns):
    sys.exit(f"[ERROR] CSV must contain columns: {expected_columns}")

# Validate BAM file existence
missing = samples_df.loc[~samples_df["path_to_flnc_bam"].apply(os.path.exists), "path_to_flnc_bam"]
if not missing.empty:
    sys.exit("[ERROR] These FLNC BAM files do not exist:\n  " + "\n  ".join(missing.tolist()))

# Check for duplicate BAM paths
if samples_df["path_to_flnc_bam"].duplicated().any():
    dups = samples_df.loc[samples_df["path_to_flnc_bam"].duplicated(), "path_to_flnc_bam"]
    sys.exit("[ERROR] Duplicate FLNC BAM paths found:\n  " + "\n  ".join(dups.tolist()))

print("[INFO] Sample sheet validation passed.", file=sys.stderr)


##############################
# LOCAL VARIABLE DEFINITIONS #
##############################

# Utility function to make sure that values we expect to be booleans are passed as such
def as_bool(val):
    if isinstance(val, bool):
        return val
    if isinstance(val, str):
        return val.strip().lower() in {"true", "yes", "1"}
    return False

# Set snakemake main workdir variable. 
SNAKEDIR = os.path.dirname(workflow.snakefile) + "/"

# Group BAMs by sample
FLNC_BAMS = defaultdict(list)
for row in samples_df.itertuples(index=False):
    FLNC_BAMS[row.sample].append(row.path_to_flnc_bam)

# Create flat map of (sample, part) -> bam_path
FLNC_BAM_PARTS = {
    (sample, i): bam
    for sample, bam_list in FLNC_BAMS.items()
    for i, bam in enumerate(bam_list)
}

# Define local variables: General
SAMPLES         = sorted(FLNC_BAMS.keys())
MERGEDISOPREFIX = config.get("merged_isoform_prefix", "ISOP")
MAXINTRONLEN    = config.get("max_intron_length", 1250000)
USE_TC          = as_bool(config.get("use_transcriptclean", True))
ONLYCHR         = as_bool(config.get("limit_output_to_chrNXY", True))

# Define local variables: Reference files
GENOMEFASTA     = config.get("reference_genome_fasta")
REFGTF          = config.get("reference_annotations_gtf")
REFTSS          = config.get("reference_tss_bed")
SPLICEJUNCTIONS = config.get("splice_junctions_file")
TERMINALEXONBED = config.get("reference_intrapriming_exclude_regions_bed","")
RMSKBED         = config.get("reference_repeat_regions_bed")
PARBED          = config.get("reference_PAR_regions_bed")
SEGDUP          = config.get("reference_segdups_bed")

# Define local variables: Filter thresholds
FILT_MONOEXON_TSS_MAXDIST_BP      = config.get("filt_monoexon_tss_maxdist_bp", 10)
FILT_MONOEXON_MIN_INTRON_OVLP_BP  = config.get("filt_monoexon_min_intron_ovlp_bp", 10)
FILT_RMSK_MIN_OVLP_FRACT          = config.get("filt_rmsk_min_ovlp_fract", 0.9)
FILT_PAR_MIN_OVLP_FRACT           = config.get("filt_par_min_ovlp_fract", 0)
FILT_TPM_MIN_COUNT                = config.get("filt_tpm_min_count", 10)
FILT_TPM_MIN_FRACT                = config.get("filt_tpm_min_fract", 0.5)

# Define local variables: Choose whether or not to run certain rules
REMOVE_MONOEXONS              = as_bool(config.get("remove_monoexons_without_cage_support", True))
REMOVE_MONOEXON_FRAGMENTS     = as_bool(config.get("remove_monoexon_premRNA_fragments", True))
REMOVE_NONCANONICAL_SPLICE    = as_bool(config.get("remove_isoforms_with_noncanonical_splice_junctions", True))
REMOVE_TSWITCH_ARTIFACTS      = as_bool(config.get("remove_isoforms_with_template_switching_artifacts", True))
REMOVE_ANTISENSE_SPLICEMATCH  = as_bool(config.get("remove_isoforms_with_antisense_perfect_splicematch", True))
REMOVE_CONTAINED_IN_REPEATS   = as_bool(config.get("remove_isoforms_fully_contained_in_repeats", True))
REMOVE_PAR_OVERLAP            = as_bool(config.get("remove_isoforms_overlapping_PAR_regions", True))
REMOVE_BELOW_TPM              = as_bool(config.get("remove_isoforms_not_meeting_TPM_threshold", True))

# Set conditional isoPropeller arguments. If a terminal exon bed file is defined
# in the configuration file, use it.
if TERMINALEXONBED:
    ISOPROPEXTRAARGS = f"-l {TERMINALEXONBED}"
else:
    ISOPROPEXTRAARGS = ""


#########
# RULES #
#########

# Import common rules
include: "rules/general_rules.smk"
include: "rules/01_mapping.smk"
include: "rules/03_isopropeller.smk"
include: "rules/04_isopropeller-merge.smk"
include: "rules/05_isopropeller-filter.smk"

# Import conditional rules
if USE_TC:
    print("[INFO] Loading conditional transcriptclean rules.", file=sys.stderr)
    include: "rules/02_transcriptclean.smk"

# Make final targets
rule all:
    input:
        expand("01_mapping/{sample}/{sample}_mapped_labeled.bam", sample=SAMPLES),
        expand("02_transcriptclean/{sample}/{sample}_mapped_labeled_tclean.bam", sample=SAMPLES),
        expand("03_isoPropeller/{sample}/{sample}_all.gtf", sample=SAMPLES),
        expand("03_isoPropeller/{sample}/{sample}_depth-gt1.gtf", sample=SAMPLES),
        expand("04_isoPropeller-merge/{prefix}_{suffix}.gtf", prefix=MERGEDISOPREFIX, suffix=["all", "depth-gt1"]),
        expand("04_isoPropeller-merge/{prefix}_{suffix}_{end}.bed", prefix=MERGEDISOPREFIX, suffix=["all", "depth-gt1"], end=["tss", "tts"]),
        expand("05_isoPropeller-filter/{prefix}_depth-gt1_" + FILTERTAG + "/filtered_{prefix}_depth-gt1.gtf", prefix=[MERGEDISOPREFIX]),
        expand("05_isoPropeller-filter/{prefix}_depth-gt1_" + FILTERTAG + "/filtered_{prefix}_depth-gt1_exp.txt", prefix=[MERGEDISOPREFIX]),
        expand("05_isoPropeller-filter/{prefix}_depth-gt1_" + FILTERTAG + "/filtered_{prefix}_depth-gt1_id.txt", prefix=[MERGEDISOPREFIX]),
        expand("05_isoPropeller-filter/{prefix}_depth-gt1_" + FILTERTAG + "/filtered_{prefix}_depth-gt1_tss.bed", prefix=[MERGEDISOPREFIX]),
        expand("05_isoPropeller-filter/{prefix}_depth-gt1_" + FILTERTAG + "/filtered_{prefix}_depth-gt1_tts.bed", prefix=[MERGEDISOPREFIX])
